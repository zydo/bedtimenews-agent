"""
LangGraph-based Agentic RAG agent for BedtimeNews knowledge base.

Public API:
    agent_query(question: str) -> dict
        Process a user question and return complete result.

    agent_stream_query(question: str) -> AsyncIterator[dict]
        Process a user question and yield streaming events.

Usage:
    from agent import agent_query, agent_stream_query

    # Synchronous query
    result = agent_query("How much debt in Du Shan's county?")
    print(result["answer"])

    # Streaming query
    async for event in agent_stream_query("What is the Hengshui model?"):
        if event["type"] == "token":
            print(event["content"], end="")
"""

from typing import Any, AsyncIterator

from langchain_core.messages.ai import AIMessageChunk

from .graph import AgentState, create_initial_state, graph

# ============================================================================
# Public API
# ============================================================================


def agent_query(question: str) -> dict:
    """
    Process a user question through the agentic workflow.

    Args:
        question: User's question

    Returns:
        Dict with only the answer field
    """
    initial_state: AgentState = create_initial_state(question)
    final_state = graph.invoke(input=initial_state)
    return {"answer": final_state.get("final_answer", "")}


async def agent_stream_query(question: str) -> AsyncIterator[dict[str, Any]]:
    """
    Stream answer chunks from the agentic RAG workflow as the LLM generates responses.

    This function executes the LangGraph workflow and streams only the relevant
    LLM-generated content from the final answer generation nodes. It filters
    out internal graph events and only yields actual answer content.

    Args:
        question: The user's input question or query string

    Yields:
        dict: Answer chunk events with structure:
            {
                "type": "answer_chunk",
                "content": "text content from LLM generation"
            }

    Workflow:
        1. Creates initial state with the question
        2. Streams events from the compiled LangGraph workflow
        3. Filters events to only include:
           - Stream events ("on_chat_model_stream")
           - From answer generation nodes ("direct" or "generate")
           - AI message chunks (AIMessageChunk instances)
        4. Yields formatted answer chunks

    Filtering Logic:
        - Only processes "on_chat_model_stream" events (ignores other graph events)
        - Only includes content from "direct" (greeting responses) and "generate" (RAG answers) nodes
        - Only yields AIMessageChunk instances (ignores other message types)

    Example:
        async for chunk in agent_stream_query("What is the capital of France?"):
            print(chunk)  # {"type": "answer_chunk", "content": "Paris is the capital..."}

    Note:
        This function is designed to work with streaming chat interfaces and
        should be consumed by functions that format the output (e.g., SSE formatting).
    """
    initial_state = create_initial_state(question)
    async for event in graph.astream_events(initial_state, version="v2"):
        # Only yield streams, ignore other graph-internal events
        if event.get("event", "") != "on_chat_model_stream":
            continue

        # Only yield the reply generating nodes' output, ignore other nodes'
        langgraph_node = event.get("metadata", {}).get("langgraph_node", "")
        if langgraph_node != "direct" and langgraph_node != "generate":
            continue

        # Only yield messages generated by LLM
        chunk = event.get("data", {}).get("chunk")
        if not isinstance(chunk, AIMessageChunk):
            continue

        yield {"type": "answer_chunk", "content": chunk.content}
